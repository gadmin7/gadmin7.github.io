<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Billion-Word Hack: How Word2Vec‚Äôs Speed Secret Created a Hidden Geometric Flaw</title>
  <link rel="stylesheet" href="../css/styles.css">
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0 auto; max-width: 900px; padding: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    h1 { border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-bottom: 30px; }
    pre { background: #f4f4f4; padding: 10px; border-left: 3px solid #007bff; overflow-x: auto; }
    .analogy { background-color: #eaf6ff; padding: 15px; border-radius: 5px; margin: 15px 0; border: 1px solid #c9e6ff;}
    table { width: 100%; border-collapse: collapse; margin: 20px 0; }
    th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
    .reference-list { list-style-type: none; padding-left: 0; }
    .reference-list li { margin-bottom: 15px; padding-left: 20px; text-indent: -20px; }
    .ref-authors { font-weight: bold; }
    .math-block { margin: 20px 0; text-align: center; }
    .blog-image { max-width: 100%; height: auto; border: 1px solid #ccc; margin-bottom: 10px; }
    .definition-box {
      background: #f9f9f9;
      border-left: 4px solid #3498db;
      padding: 10px 15px;
      margin: 20px 0;
      font-size: 0.95em;
    }
  </style>
</head>

<body>
<header id="main-header">
  <nav style="margin-bottom: 20px;">
    <a href="../index.html">‚Üê Portfolio Home</a> | 
    <a href="../myblogs.html">Blog</a>
  </nav>
</header>

<article>
  <h1>The Billion-Word Hack: How Word2Vec‚Äôs Speed Secret Created a Hidden Geometric Flaw</h1>
  <p style="color: #666; font-style: italic;">Published: November 9, 2025 </p>
  <hr>

  <!-- PART I -->
  <section id="part-1">
    <h2>üß© Part I: The Computational Crisis and the Brilliant Hack</h2>

    <h3>The Goal: Perfect Word Geometry</h3>
    <p>For decades, NLP‚Äôs holy grail was to encode a word‚Äôs meaning into a numerical vector such that <strong>Vector Similarity ‚âà Semantic Similarity</strong>. The closer two vectors in space, the more related their meanings.</p>

    <h3>The Solution: The Skip-Gram Model</h3>
    <figure style="text-align: center; margin: 20px 0;">
      <img src="../images/skip_gram.png" alt="Skip-Gram Diagram" class="blog-image">
      <figcaption>Figure 1: The Skip-Gram training objective predicts surrounding words from a center word.</figcaption>
    </figure>
    <p>The Skip-Gram model reframed language modeling as a prediction problem: Given a center word ($w_t$), predict its surrounding context words ($w_{t+j}$). The objective is to maximize:</p>

    <div class="math-block">
      $$\frac{1}{T}\sum_{t=1}^{T}\sum_{-c \le j \le c, j \ne 0}\log p(w_{t+j} | w_t) \tag{1}$$
    </div>

    <p>The conditional probability is modeled using the <strong>Softmax</strong> function:</p>
    <div class="math-block">
      $$p(w_O|w_I) = \frac{\exp((v'_{w_O})^\top v_{w_I})} {\sum_{w=1}^{|V|} \exp((v'_w)^\top v_{w_I})} \tag{2}$$
    </div>
    <p>Here, $v_w$ and $v'_w$ are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations of $w$, and $|V|$ is the vocabulary size.</p>

    <h3>The Bottleneck: The Softmax Tsunami</h3>
    <p>Computing this softmax denominator scales with vocabulary size ($|V|$ often 10‚Åµ‚Äì10‚Å∑), leading to billions of multiplications per update. Training on billion-word corpora was computationally prohibitive.</p>

    <h3>The SGNS Hack: Trading Purity for Speed</h3>
    <p>Tomas Mikolov and colleagues introduced <strong>Skip-Gram with Negative Sampling (SGNS)</strong>, an approximation that replaces the softmax with $K+1$ binary logistic regressions:</p>
    <div class="math-block">
      $$L = \log \sigma({v'_{w_O}}^\top v_{w_I}) + \sum_{i=1}^{K} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-{v'_{w_i}}^\top v_{w_I}) \right] \tag{3}$$
    </div>
    <p>Here, negative samples $w_i$ are drawn from a noise distribution $P_n(w)$ (typically $U(w)^{3/4}$). This trick reduced per-update complexity from $O(|V|)$ to $O(K)$ a breakthrough for scalability.</p>

    <h3>The Binary Decision Layer: Two Sets of Vectors</h3>
    <p>SGNS learns two distinct embedding matrices: <strong>input vectors $v_w$</strong> (words as predictors) and <strong>output vectors $v'_w$</strong> (words as predicted contexts). Most downstream tasks use the input embeddings $v_w$.</p>
  </section>

  <hr>

  <!-- PART II -->
  <section id="part-2">
    <h2>üìê Part II: The Unintended Consequence ‚ÄúThe Narrow Cone‚Äù</h2>
    <p>In 2017, <a href="https://aclanthology.org/D17-1308/" target="_blank">Mimno & Thompson</a> revealed a geometric anomaly: SGNS embeddings occupy a <strong>narrow cone</strong> in space rather than an isotropic (uniform) distribution.</p>

    <div class="definition-box">
      <strong>Terminology:</strong><br>
      ‚Ä¢ <em>Isotropic</em> ‚Äì vectors are uniformly distributed across all directions.<br>
      ‚Ä¢ <em>Anisotropic</em> ‚Äì vectors cluster in a limited region of space.<br>
      ‚Ä¢ <em>Narrow cone</em> ‚Äì most vectors share a similar direction, reducing angular diversity.
    </div>

    <h3>The Strange Geometry</h3>
    <ul>
      <li>Word vectors align along a dominant axis, forming a narrow cone.</li>
      <li>Most input vectors are non-negative in many dimensions.</li>
      <li>Context vectors tend to point in the opposite direction.</li>
    </ul>
    <figure style="text-align: center; margin: 20px 0;">
      <img src="../images/SGNS_geometry.jpg" alt="SGNS Geometry" class="blog-image">
      <figcaption>Figure 2: The narrow cone phenomenon (Mimno & Thompson, 2017).</figcaption>
    </figure>

    <h3>The Cost to Cosine Similarity</h3>
    <p>Because vectors share a common global bias direction, cosine similarity becomes less discriminative it measures small angular deviations rather than true semantic distance. However, as Mimno & Thompson note, SGNS still performs well on many tasks despite this anisotropy. The geometry is thus <em>distorted but functional</em>, not ‚Äúbroken.‚Äù</p>
  </section>


  <!-- Bridge -->
  <section id="bridge">
    <p>This geometric distortion raises deeper questions: Why does an efficient optimization trick yield such a skewed representation and what does that say about how AI systems pursue proxy goals?</p>
  </section>

  <hr>

  <!-- PART III -->
  <section id="part-3">
    <h2>üß≠ Part III: The Meta-Lesson Outer Misalignment (Reward Misspecification)</h2>
    <p>The SGNS story mirrors <strong>outer misalignment</strong> in AI safety: a system optimizes the literal objective but diverges from the intended goal.</p>

    <table>
      <thead><tr><th>Concept</th><th>Applied to SGNS</th></tr></thead>
      <tbody>
        <tr><td><strong>Intended Goal</strong></td><td>Learn a semantically faithful, isotropic embedding space.</td></tr>
        <tr><td><strong>Proxy Objective</strong></td><td>Efficiently minimize the negative sampling loss ($O(K)$ complexity).</td></tr>
        <tr><td><strong>Outcome</strong></td><td>Model finds a shortcut: collapse vectors into a narrow cone that still minimizes loss but degrades geometry.</td></tr>
      </tbody>
    </table>

    <p>This is analogous to an AI system exploiting an imperfect reward signal. SGNS didn‚Äôt ‚Äúchoose‚Äù anisotropy but its optimization process discovered a local minimum that satisfies the proxy efficiently.</p>
  </section>

  <hr>

  <!-- PART IV -->
  <section id="part-4">
    <h2>‚öñÔ∏è Part IV: Why Geometry Matters for Fairness and Bias</h2>
    <p>Linear debiasing techniques (e.g., <em>subspace projection</em> $v_{\text{new}} = v - (v \cdot g)g$) assume an approximately linear embedding manifold. SGNS‚Äôs anisotropic cone violates this assumption.</p>

    <h3>üìâ Entangled Bias</h3>
    <p>Because negative sampling uses $P_n(w) \propto U(w)^{3/4}$, common words dominate updates, coupling word frequency bias with gender or cultural biases. This <strong>entangles bias across the embedding space</strong>, making it nonlinear and harder to remove with projection methods (<a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" target="_blank">Bolukbasi et al., 2016</a>).</p>

    <p>Thus, anisotropy doesn‚Äôt just distort geometry it also <strong>complicates fairness interventions</strong> that assume simple linear relationships.</p>
    <h3>üß∞ Mitigations</h3>
    <ul>
        <li><strong>Post-processing for isotropy:</strong> Remove dominant principal components (‚Äúall-but-the-top‚Äù, <a href="https://arxiv.org/pdf/1702.01417" target="_blank">Mu & Viswanath, 2018</a>).</li>
      <li><strong>Alternative training objectives:</strong> GloVe and FastText often yield more balanced distributions.</li>
      <li><strong>Regularization:</strong> Penalize mean vector magnitude or encourage variance along all dimensions.</li>
      <li><strong>Evaluate anisotropy metrics:</strong> Track angular variance during training to detect collapse early.</li>
    </ul>
    <p>These methods don‚Äôt eliminate bias or misalignment, but they demonstrate that geometric flaws can be diagnosed and mitigated through design awareness.</p>
  </section>

  <hr>

  <!-- PART VI -->
  <section id="part-6">
    <h2>üö® Part V: Lessons for General AI Safety and Interpretability</h2>
    <h3>1. The Tyranny of the Proxy</h3>
    <p>Whenever a complex goal is replaced with a measurable proxy, systems optimize the proxy‚Äîeven if it subverts the true goal. SGNS‚Äôs narrow cone is the efficient shortcut for its loss function.</p>

    <h3>2. Interpretability and Asymmetry</h3>
    <p>SGNS produces two asymmetric spaces ($v_w$, $v'_w$), showing that efficient optimization often warps internal representations. Understanding such internal ‚Äúgeometry drift‚Äù is key for interpretability and trustworthy AI design.</p>
  </section>

  <hr>

  <!-- REFERENCES -->
  <section id="references">
    <h2>üìö References</h2>
    <ul class="reference-list">
      <li><span class="ref-authors">1. Mikolov et al. (2013).</span> <strong>Distributed Representations of Words and Phrases and their Compositionality</strong>. <a href="https://arxiv.org/pdf/1310.4546" target="_blank">PDF</a></li>
      <li><span class="ref-authors">2. Mimno & Thompson (2017).</span> <strong>The Strange Geometry of Skip-Gram with Negative Sampling</strong>. <a href="https://aclanthology.org/D17-1308/" target="_blank">ACL Anthology</a></li>
      <li><span class="ref-authors">3. Bolukbasi et al. (2016).</span> <strong>Man is to Computer Programmer as Woman is to
        Homemaker? Debiasing Word Embeddings</strong>. <a href="https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" target="_blank">PDF</a></li>
      <li><span class="ref-authors">4. Mu & Viswanath (2018).</span> <strong>All-but-the-Top: Simple and Effective Post-Processing for Word Representations</strong>. <a href="https://arxiv.org/pdf/1702.01417" target="_blank">arXiv</a></li>
      <li><span class="ref-authors">5. Ethayarajh et al. (2019).</span> <strong>Understanding Undesirable Word Embedding Associations</strong>. <a href="https://aclanthology.org/P19-1065/" target="_blank">ACL Anthology</a></li>
    </ul>
  </section>
</article>

<script>
MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] } };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
