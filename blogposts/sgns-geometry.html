<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Billion-Word Hack: How Word2Vec‚Äôs Speed Secret Shaped Its Geometry</title>
  <link rel="stylesheet" href="../css/styles.css">
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0 auto; max-width: 900px; padding: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    h1 { border-bottom: 2px solid #3498db; padding-bottom: 10px; margin-bottom: 30px; }
    pre { background: #f4f4f4; padding: 10px; border-left: 3px solid #007bff; overflow-x: auto; }
    .analogy { background-color: #eaf6ff; padding: 15px; border-radius: 5px; margin: 15px 0; border: 1px solid #c9e6ff;}
    table { width: 100%; border-collapse: collapse; margin: 20px 0; }
    th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
    .reference-list { list-style-type: none; padding-left: 0; }
    .reference-list li { margin-bottom: 15px; padding-left: 20px; text-indent: -20px; }
    .ref-authors { font-weight: bold; }
    .math-block { margin: 20px 0; text-align: center; }
    .blog-image { max-width: 100%; height: auto; border: 1px solid #ccc; margin-bottom: 10px; }
    .definition-box {
      background: #f9f9f9;
      border-left: 4px solid #3498db;
      padding: 10px 15px;
      margin: 20px 0;
      font-size: 0.95em;
    }
    .disclaimer {
      font-style: italic;
      font-size: 0.9em;
      color: #555;
      background: #fff8e1;
      border-left: 4px solid #f1c40f;
      padding: 10px 15px;
      margin: 15px 0;
    }
  </style>
</head>

<body>
<header id="main-header">
  <nav style="margin-bottom: 20px;">
    <a href="../index.html">‚Üê Portfolio Home</a> | 
    <a href="../myblogs.html">Blog</a>
  </nav>
</header>

<article>
  <h1>The Billion-Word Hack: How Word2Vec‚Äôs Speed Secret Shaped Its Geometry</h1>
  <p style="color: #666; font-style: italic;">Published: November 9, 2025 </p>
  <hr>

  <!-- PART I -->
  <section id="part-1">
    <h2>üß© Part I: The Computational Crisis and the Brilliant Hack</h2>

    <h3>The Goal: Perfect Word Geometry</h3>
    <p>A long-standing goal in NLP was to represent words as vectors so that geometric closeness correlates with semantic relatedness. We wanted 
        to encode a word‚Äôs meaning into a numerical vector such that <strong>Vector Similarity ‚âà Semantic Similarity</strong>. The closer two vectors in space, 
        the more related their meanings.</p>

    <h3>The Solution: The Skip-Gram Model</h3>
    <figure style="text-align: center; margin: 20px 0;">
      <img src="../images/skip_gram.png" alt="Skip-Gram Diagram" class="blog-image">
      <figcaption>Figure 1: The Skip-Gram training objective predicts surrounding words from a center word.</figcaption>
    </figure>
    <p>The Skip-Gram model reframed language modeling as a prediction problem: Given a center word ($w_t$), predict its surrounding context words ($w_{t+j}$). The objective is to maximize:</p>

    <div class="math-block">
      $$\frac{1}{T}\sum_{t=1}^{T}\sum_{-c \le j \le c, j \ne 0}\log p(w_{t+j} | w_t) \tag{1}$$
    </div>

    <p>The conditional probability is modeled using the <strong>Softmax</strong> function:</p>
    <div class="math-block">
      $$p(w_O|w_I) = \frac{\exp((v'_{w_O})^\top v_{w_I})} {\sum_{w=1}^{|V|} \exp((v'_w)^\top v_{w_I})} \tag{2}$$
    </div>
    <p>Here, $v_w$ and $v'_w$ are the ‚Äúinput‚Äù and ‚Äúoutput‚Äù vector representations of $w$, and $|V|$ is the vocabulary size.
        We will later use only $v_w$ for downstream tasks.</p>

    <h3>The Bottleneck: The Softmax Tsunami</h3>
    <p>Computing this softmax denominator scales with vocabulary size ($|V|$ often 10‚Åµ‚Äì10‚Å∑), leading to billions of multiplications per update. Training with the full softmax on billion-word corpora was expensive.</p>

    <h3>The SGNS Hack: Trading Purity for Speed</h3>
    <p>Tomas Mikolov and colleagues introduced <strong>Skip-Gram with Negative Sampling (SGNS)</strong>, an approximation that replaces the softmax with $K+1$ binary logistic regressions:</p>
    <div class="math-block">
      $$L = \log \sigma({v'_{w_O}}^\top v_{w_I}) + \sum_{i=1}^{K} \mathbb{E}_{w_i \sim P_n(w)} \left[ \log \sigma(-{v'_{w_i}}^\top v_{w_I}) \right] \tag{3}$$
    </div>
    <p>Here, negative samples $w_i$ are drawn from a noise distribution $P_n(w)$ (typically $U(w)^{3/4}$). This trick reduced per-update complexity from $O(|V|)$ to $O(K)$‚Äîa breakthrough for scalability.</p>

    <h3>The Binary Decision Layer: Two Sets of Vectors</h3>
    <p>SGNS learns two distinct embedding matrices: <strong>input vectors $v_w$</strong> (words as predictors) and <strong>output vectors $v'_w$</strong> (words as predicted contexts). Most downstream tasks use the input embeddings $v_w$.</p>
  </section>

  <hr>

  <!-- PART II -->
  <section id="part-2">
    <h2>üìê Part II: The Unintended Consequence ‚Äî ‚ÄúThe Narrow Cone‚Äù</h2>
    <p>In 2017, <a href="https://aclanthology.org/D17-1308/" target="_blank">Mimno & Thompson</a> revealed a geometric anomaly: SGNS embeddings often occupy a <strong>narrow cone</strong> in space rather than an isotropic (uniform) distribution.</p>

    <div class="definition-box">
      <strong>Terminology:</strong><br>
      ‚Ä¢ <em>Isotropic</em> ‚Äì vectors are uniformly distributed across all directions.<br>
      ‚Ä¢ <em>Anisotropic</em> ‚Äì vectors cluster in a limited region of space.<br>
      ‚Ä¢ <em>Narrow cone</em> ‚Äì most vectors share a similar direction, reducing angular diversity.
    </div>

    <h3>In the Strange Geometry paper they found:</h3>
    <ul>
      <li>Word vectors align along a dominant axis, forming a narrow cone.</li>
      <li>Most input vectors are non-negative in many dimensions.</li>
      <li>Context vectors tend to point in the opposite direction.</li>
    </ul>

    <figure style="text-align: center; margin: 20px 0;">
      <img src="../images/sgns_pca.png" alt="SGNS Geometry" class="blog-image">
      <figcaption>Figure 2: SGNS word vectors and their context vectors
        projected using PCA (left) and t-SNE (right). t-SNE provides
        a more readable layout, but masks the divergence between
        word and context vectors. (Mimno & Thompson, 2017).</figcaption>
    </figure>

    <p>Because vectors share a common global bias direction, cosine similarity becomes less discriminative‚Äîit measures small angular deviations rather than large semantic ones. However, as Mimno & Thompson note, SGNS still performs well on many tasks despite this anisotropy. The geometry is thus <em>distorted but functional</em>, not ‚Äúbroken.‚Äù</p>
    <h3>Why Geometry Matters</h3>
    <p>The geometry of an embedding space directly determines how similarity, clustering, and analogy operations behave. 
       In an isotropic space, cosine similarity captures meaningful relational differences because vectors are spread evenly across directions. 
       In an anisotropic or narrow-cone space, many vectors share similar orientations, making pairwise cosine similarities artificially high even for unrelated words. 
       This can blur semantic distinctions, hurt interpretability, and make downstream classifiers or nearest-neighbor retrieval less sensitive to genuine differences in meaning.</p>

    <p>Researchers have found that post-processing steps like removing top principal components or whitening the space 
       (<a href="https://arxiv.org/pdf/1702.01417" target="_blank">Mu & Viswanath, 2018</a>) can partially restore isotropy and improve performance on similarity and analogy benchmarks. 
       This shows that <strong>geometry isn‚Äôt just cosmetic it encodes how meaning is organized and compared</strong> in the model‚Äôs latent space.</p>

    <div class="disclaimer">
      These geometric effects are empirical and depend on training hyperparameters (e.g., negative sample ratio). Read the original paper for experimental details and assumptions.
    </div>
  </section>

  <hr> 
  <section id="bridge">
    <p>The geometry is thus <em>distorted but functional</em>, not ‚Äúbroken‚Äù suggests the optimization objective might be encouraging an easier-to-satisfy configuration than the one we intended. This geometric distortion raises deeper questions: Why does an efficient optimization trick yield such a skewed representation‚Äîand what does that say about how AI systems pursue proxy goals?</p>
  </section>


  <!-- PART III -->
  <section id="part-3">
    <h2>üß≠ Part III: An Analogy to Outer Misalignment (Reward Misspecification)</h2>
    <p>The SGNS story serves as an <strong>analogy</strong>‚Äînot a direct equivalence‚Äîto <strong>outer misalignment</strong> in AI safety: a system optimizes the literal objective but diverges from the intended goal.</p>

    <table>
      <thead><tr><th>Concept</th><th>Applied to SGNS</th></tr></thead>
      <tbody>
        <tr><td><strong>Intended Goal</strong></td><td>Learn embeddings where distances reflect semantics well enough for downstream tasks.</td></tr>
        <tr><td><strong>Proxy Objective</strong></td><td>Efficiently minimize the negative sampling loss ($O(K)$ complexity).</td></tr>
        <tr><td><strong>Observed Outcome</strong></td><td>A representation that performs well but exhibits anisotropy.</td></tr>
        <tr><td><strong>Revised Goal (what researchers later wanted)</strong></td><td>Learn a semantically faithful, more isotropic embedding space.</td></tr>
      </tbody>
    </table>

    <div class="analogy">
      <strong>Analogy note:</strong> SGNS didn‚Äôt ‚Äúchoose‚Äù anisotropy; rather, its optimization process found a configuration that satisfied the proxy loss efficiently. This mirrors, in a metaphorical sense, how AI systems might over-optimize proxies that diverge from designer intent.
    </div>
  </section>

  <hr>

  <!-- PART IV -->
  <section id="part-6">
    <h2>üö® Part IV: Lessons for Optimization and Representation</h2>
    <h3>1. The Tyranny of the Proxy</h3>
    <p>Whenever a complex goal is replaced with a measurable proxy, systems might optimize the proxy even if it subverts the true goal. SGNS‚Äôs narrow cone might be the efficient shortcut for its loss function.</p>

    <h3>2. Interpretability and Asymmetry</h3>
    <p>SGNS produces two asymmetric spaces ($v_w$, $v'_w$), showing that efficiency-driven optimization can warp internal representations. Understanding such internal ‚Äúgeometry drift‚Äù is crucial for interpretability and reliable model design.</p>

    <p>The SGNS geometry reminds us that computational shortcuts can reshape representation space itself‚Äîa useful cautionary story for both NLP and broader AI systems.</p>
  </section>

  <hr>

  <!-- REFERENCES -->
  <section id="references">
    <h2>üìö References</h2>
    <ul class="reference-list">
      <li><span class="ref-authors">1. Mikolov et al. (2013).</span> <strong>Distributed Representations of Words and Phrases and their Compositionality</strong>. <a href="https://arxiv.org/pdf/1310.4546" target="_blank">PDF</a></li>
      <li><span class="ref-authors">2. Mimno & Thompson (2017).</span> <strong>The Strange Geometry of Skip-Gram with Negative Sampling</strong>. <a href="https://aclanthology.org/D17-1308/" target="_blank">ACL Anthology</a></li>
      <li><span class="ref-authors">3. Mu & Viswanath (2018).</span> <strong>All-but-the-Top: Simple and Effective Post-Processing for Word Representations</strong>. <a href="https://arxiv.org/pdf/1702.01417" target="_blank">arXiv</a></li>
      <li><span class="ref-authors">4. Ethayarajh et al. (2019).</span> <strong>Understanding Undesirable Word Embedding Associations</strong>. <a href="https://aclanthology.org/P19-1166/" target="_blank">ACL Anthology</a></li>
    </ul>
  </section>
</article>

<script>
MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] } };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
